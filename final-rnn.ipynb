{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forex Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ishadi/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from math import sqrt\n",
    "from numpy import concatenate\n",
    "from matplotlib import pyplot\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert series to supervised learning\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "   n_vars = 1 if type(data) is list else data.shape[1]\n",
    "   df = DataFrame(data)\n",
    "   cols, names = list(), list()\n",
    "   # input sequence (t-n, ... t-1)\n",
    "   for i in range(n_in, 0, -1):\n",
    "      cols.append(df.shift(i))\n",
    "      names += [('var%d(t-%d)' % (j + 1, i)) for j in range(n_vars)]\n",
    "   # forecast sequence (t, t+1, ... t+n)\n",
    "   for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "         names += [('var%d(t)' % (j + 1)) for j in range(n_vars)]\n",
    "      else:\n",
    "         names += [('var%d(t+%d)' % (j + 1, i)) for j in range(n_vars)]\n",
    "   # put it all together\n",
    "   agg = concat(cols, axis=1)\n",
    "   agg.columns = names\n",
    "\n",
    "   # drop rows with NaN values\n",
    "   if dropnan:\n",
    "     agg.dropna(inplace=True)\n",
    "   return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tailer\n",
    "import tailer as tl\n",
    "import pandas as pd\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get the latest 200 records from the frequently updating data file\n",
    "# file = open('training_set.txt')\n",
    "# lastLines = tl.tail(file,200) #to read last 200 lines\n",
    "# file.close()\n",
    "# dataset=pd.read_csv(io.StringIO('\\n'.join(lastLines)),index_col=0)\n",
    "# values = dataset.values\n",
    "# print(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "dataset = read_csv('Output.txt',index_col=0)  #, nrows=5\n",
    "dataset = dataset[-100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1.13806</th>\n",
       "      <th>1.13805</th>\n",
       "      <th>1.13806.1</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1541178970</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1541184970</th>\n",
       "      <td>1.13853</td>\n",
       "      <td>1.13853</td>\n",
       "      <td>1.13853</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1541185030</th>\n",
       "      <td>1.13836</td>\n",
       "      <td>1.13836</td>\n",
       "      <td>1.13836</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1541185090</th>\n",
       "      <td>1.13828</td>\n",
       "      <td>1.13828</td>\n",
       "      <td>1.13828</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1541185150</th>\n",
       "      <td>1.13841</td>\n",
       "      <td>1.13841</td>\n",
       "      <td>1.13842</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1541185210</th>\n",
       "      <td>1.13928</td>\n",
       "      <td>1.13928</td>\n",
       "      <td>1.13928</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1541185270</th>\n",
       "      <td>1.13916</td>\n",
       "      <td>1.13916</td>\n",
       "      <td>1.13917</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1541185330</th>\n",
       "      <td>1.13911</td>\n",
       "      <td>1.13911</td>\n",
       "      <td>1.13911</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1541185390</th>\n",
       "      <td>1.13935</td>\n",
       "      <td>1.13935</td>\n",
       "      <td>1.13935</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1541185450</th>\n",
       "      <td>1.13933</td>\n",
       "      <td>1.13933</td>\n",
       "      <td>1.13933</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1541185510</th>\n",
       "      <td>1.13912</td>\n",
       "      <td>1.13912</td>\n",
       "      <td>1.13912</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1541185570</th>\n",
       "      <td>1.13901</td>\n",
       "      <td>1.13901</td>\n",
       "      <td>1.13901</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1541185630</th>\n",
       "      <td>1.13891</td>\n",
       "      <td>1.13891</td>\n",
       "      <td>1.13891</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1541185690</th>\n",
       "      <td>1.13899</td>\n",
       "      <td>1.13899</td>\n",
       "      <td>1.13899</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1541185750</th>\n",
       "      <td>1.13878</td>\n",
       "      <td>1.13878</td>\n",
       "      <td>1.13878</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1541185810</th>\n",
       "      <td>1.13891</td>\n",
       "      <td>1.13891</td>\n",
       "      <td>1.13891</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1541185870</th>\n",
       "      <td>1.13886</td>\n",
       "      <td>1.13886</td>\n",
       "      <td>1.13886</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1541185930</th>\n",
       "      <td>1.13867</td>\n",
       "      <td>1.13867</td>\n",
       "      <td>1.13867</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1541185990</th>\n",
       "      <td>1.13873</td>\n",
       "      <td>1.13873</td>\n",
       "      <td>1.13873</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1541186050</th>\n",
       "      <td>1.13881</td>\n",
       "      <td>1.13880</td>\n",
       "      <td>1.13881</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1541186110</th>\n",
       "      <td>1.13885</td>\n",
       "      <td>1.13885</td>\n",
       "      <td>1.13885</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1541186170</th>\n",
       "      <td>1.13879</td>\n",
       "      <td>1.13879</td>\n",
       "      <td>1.13879</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1541186230</th>\n",
       "      <td>1.13888</td>\n",
       "      <td>1.13888</td>\n",
       "      <td>1.13888</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1541186290</th>\n",
       "      <td>1.13891</td>\n",
       "      <td>1.13890</td>\n",
       "      <td>1.13891</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1541186350</th>\n",
       "      <td>1.13891</td>\n",
       "      <td>1.13891</td>\n",
       "      <td>1.13891</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1541186410</th>\n",
       "      <td>1.13898</td>\n",
       "      <td>1.13898</td>\n",
       "      <td>1.13898</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1541186470</th>\n",
       "      <td>1.13902</td>\n",
       "      <td>1.13901</td>\n",
       "      <td>1.13902</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1541186530</th>\n",
       "      <td>1.13916</td>\n",
       "      <td>1.13916</td>\n",
       "      <td>1.13917</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1541186590</th>\n",
       "      <td>1.13917</td>\n",
       "      <td>1.13917</td>\n",
       "      <td>1.13917</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1541186650</th>\n",
       "      <td>1.13916</td>\n",
       "      <td>1.13916</td>\n",
       "      <td>1.13917</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1541186710</th>\n",
       "      <td>1.13922</td>\n",
       "      <td>1.13922</td>\n",
       "      <td>1.13922</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1541189170</th>\n",
       "      <td>1.13963</td>\n",
       "      <td>1.13963</td>\n",
       "      <td>1.13963</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1541189230</th>\n",
       "      <td>1.13963</td>\n",
       "      <td>1.13963</td>\n",
       "      <td>1.13963</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1541189290</th>\n",
       "      <td>1.13956</td>\n",
       "      <td>1.13955</td>\n",
       "      <td>1.13956</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1541189350</th>\n",
       "      <td>1.13938</td>\n",
       "      <td>1.13937</td>\n",
       "      <td>1.13938</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1541189410</th>\n",
       "      <td>1.13942</td>\n",
       "      <td>1.13941</td>\n",
       "      <td>1.13942</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1541189470</th>\n",
       "      <td>1.13944</td>\n",
       "      <td>1.13944</td>\n",
       "      <td>1.13944</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1541189530</th>\n",
       "      <td>1.13943</td>\n",
       "      <td>1.13942</td>\n",
       "      <td>1.13943</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1541189590</th>\n",
       "      <td>1.13937</td>\n",
       "      <td>1.13937</td>\n",
       "      <td>1.13937</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1541189650</th>\n",
       "      <td>1.13942</td>\n",
       "      <td>1.13941</td>\n",
       "      <td>1.13942</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1541189710</th>\n",
       "      <td>1.13943</td>\n",
       "      <td>1.13942</td>\n",
       "      <td>1.13943</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1541189770</th>\n",
       "      <td>1.13938</td>\n",
       "      <td>1.13938</td>\n",
       "      <td>1.13939</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1541189830</th>\n",
       "      <td>1.13938</td>\n",
       "      <td>1.13938</td>\n",
       "      <td>1.13938</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1541189890</th>\n",
       "      <td>1.13942</td>\n",
       "      <td>1.13942</td>\n",
       "      <td>1.13942</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1541189950</th>\n",
       "      <td>1.13942</td>\n",
       "      <td>1.13942</td>\n",
       "      <td>1.13942</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1541190010</th>\n",
       "      <td>1.13939</td>\n",
       "      <td>1.13939</td>\n",
       "      <td>1.13940</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1541190070</th>\n",
       "      <td>1.13941</td>\n",
       "      <td>1.13940</td>\n",
       "      <td>1.13941</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1541190130</th>\n",
       "      <td>1.13943</td>\n",
       "      <td>1.13942</td>\n",
       "      <td>1.13943</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1541190190</th>\n",
       "      <td>1.13947</td>\n",
       "      <td>1.13947</td>\n",
       "      <td>1.13947</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1541190250</th>\n",
       "      <td>1.13946</td>\n",
       "      <td>1.13946</td>\n",
       "      <td>1.13947</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1541190310</th>\n",
       "      <td>1.13943</td>\n",
       "      <td>1.13943</td>\n",
       "      <td>1.13943</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1541190370</th>\n",
       "      <td>1.13946</td>\n",
       "      <td>1.13946</td>\n",
       "      <td>1.13947</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1541190430</th>\n",
       "      <td>1.13948</td>\n",
       "      <td>1.13948</td>\n",
       "      <td>1.13948</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1541190490</th>\n",
       "      <td>1.13948</td>\n",
       "      <td>1.13948</td>\n",
       "      <td>1.13948</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1541190550</th>\n",
       "      <td>1.13946</td>\n",
       "      <td>1.13946</td>\n",
       "      <td>1.13947</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1541190610</th>\n",
       "      <td>1.13947</td>\n",
       "      <td>1.13947</td>\n",
       "      <td>1.13947</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1541190670</th>\n",
       "      <td>1.13948</td>\n",
       "      <td>1.13948</td>\n",
       "      <td>1.13948</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1541190730</th>\n",
       "      <td>1.13948</td>\n",
       "      <td>1.13948</td>\n",
       "      <td>1.13948</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1541190790</th>\n",
       "      <td>1.13941</td>\n",
       "      <td>1.13941</td>\n",
       "      <td>1.13941</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1541190850</th>\n",
       "      <td>1.13936</td>\n",
       "      <td>1.13935</td>\n",
       "      <td>1.13936</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1541190910</th>\n",
       "      <td>1.13929</td>\n",
       "      <td>1.13929</td>\n",
       "      <td>1.13929</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            1.13806  1.13805  1.13806.1  0\n",
       "1541178970                                \n",
       "1541184970  1.13853  1.13853    1.13853  0\n",
       "1541185030  1.13836  1.13836    1.13836  0\n",
       "1541185090  1.13828  1.13828    1.13828  0\n",
       "1541185150  1.13841  1.13841    1.13842  0\n",
       "1541185210  1.13928  1.13928    1.13928  0\n",
       "1541185270  1.13916  1.13916    1.13917  0\n",
       "1541185330  1.13911  1.13911    1.13911  0\n",
       "1541185390  1.13935  1.13935    1.13935  0\n",
       "1541185450  1.13933  1.13933    1.13933  0\n",
       "1541185510  1.13912  1.13912    1.13912  0\n",
       "1541185570  1.13901  1.13901    1.13901  0\n",
       "1541185630  1.13891  1.13891    1.13891  0\n",
       "1541185690  1.13899  1.13899    1.13899  0\n",
       "1541185750  1.13878  1.13878    1.13878  0\n",
       "1541185810  1.13891  1.13891    1.13891  0\n",
       "1541185870  1.13886  1.13886    1.13886  0\n",
       "1541185930  1.13867  1.13867    1.13867  0\n",
       "1541185990  1.13873  1.13873    1.13873  0\n",
       "1541186050  1.13881  1.13880    1.13881  0\n",
       "1541186110  1.13885  1.13885    1.13885  0\n",
       "1541186170  1.13879  1.13879    1.13879  0\n",
       "1541186230  1.13888  1.13888    1.13888  0\n",
       "1541186290  1.13891  1.13890    1.13891  0\n",
       "1541186350  1.13891  1.13891    1.13891  0\n",
       "1541186410  1.13898  1.13898    1.13898  0\n",
       "1541186470  1.13902  1.13901    1.13902  0\n",
       "1541186530  1.13916  1.13916    1.13917  0\n",
       "1541186590  1.13917  1.13917    1.13917  0\n",
       "1541186650  1.13916  1.13916    1.13917  0\n",
       "1541186710  1.13922  1.13922    1.13922  0\n",
       "...             ...      ...        ... ..\n",
       "1541189170  1.13963  1.13963    1.13963  0\n",
       "1541189230  1.13963  1.13963    1.13963  0\n",
       "1541189290  1.13956  1.13955    1.13956  0\n",
       "1541189350  1.13938  1.13937    1.13938  0\n",
       "1541189410  1.13942  1.13941    1.13942  0\n",
       "1541189470  1.13944  1.13944    1.13944  0\n",
       "1541189530  1.13943  1.13942    1.13943  0\n",
       "1541189590  1.13937  1.13937    1.13937  0\n",
       "1541189650  1.13942  1.13941    1.13942  0\n",
       "1541189710  1.13943  1.13942    1.13943  0\n",
       "1541189770  1.13938  1.13938    1.13939  0\n",
       "1541189830  1.13938  1.13938    1.13938  0\n",
       "1541189890  1.13942  1.13942    1.13942  0\n",
       "1541189950  1.13942  1.13942    1.13942  0\n",
       "1541190010  1.13939  1.13939    1.13940  0\n",
       "1541190070  1.13941  1.13940    1.13941  0\n",
       "1541190130  1.13943  1.13942    1.13943  0\n",
       "1541190190  1.13947  1.13947    1.13947  0\n",
       "1541190250  1.13946  1.13946    1.13947  0\n",
       "1541190310  1.13943  1.13943    1.13943  0\n",
       "1541190370  1.13946  1.13946    1.13947  0\n",
       "1541190430  1.13948  1.13948    1.13948  0\n",
       "1541190490  1.13948  1.13948    1.13948  0\n",
       "1541190550  1.13946  1.13946    1.13947  0\n",
       "1541190610  1.13947  1.13947    1.13947  0\n",
       "1541190670  1.13948  1.13948    1.13948  0\n",
       "1541190730  1.13948  1.13948    1.13948  0\n",
       "1541190790  1.13941  1.13941    1.13941  0\n",
       "1541190850  1.13936  1.13935    1.13936  0\n",
       "1541190910  1.13929  1.13929    1.13929  0\n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = dataset.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.13853 1.13853 1.13853 0.     ]\n",
      " [1.13836 1.13836 1.13836 0.     ]\n",
      " [1.13828 1.13828 1.13828 0.     ]\n",
      " [1.13841 1.13841 1.13842 0.     ]\n",
      " [1.13928 1.13928 1.13928 0.     ]\n",
      " [1.13916 1.13916 1.13917 0.     ]\n",
      " [1.13911 1.13911 1.13911 0.     ]\n",
      " [1.13935 1.13935 1.13935 0.     ]\n",
      " [1.13933 1.13933 1.13933 0.     ]\n",
      " [1.13912 1.13912 1.13912 0.     ]\n",
      " [1.13901 1.13901 1.13901 0.     ]\n",
      " [1.13891 1.13891 1.13891 0.     ]\n",
      " [1.13899 1.13899 1.13899 0.     ]\n",
      " [1.13878 1.13878 1.13878 0.     ]\n",
      " [1.13891 1.13891 1.13891 0.     ]\n",
      " [1.13886 1.13886 1.13886 0.     ]\n",
      " [1.13867 1.13867 1.13867 0.     ]\n",
      " [1.13873 1.13873 1.13873 0.     ]\n",
      " [1.13881 1.1388  1.13881 0.     ]\n",
      " [1.13885 1.13885 1.13885 0.     ]\n",
      " [1.13879 1.13879 1.13879 0.     ]\n",
      " [1.13888 1.13888 1.13888 0.     ]\n",
      " [1.13891 1.1389  1.13891 0.     ]\n",
      " [1.13891 1.13891 1.13891 0.     ]\n",
      " [1.13898 1.13898 1.13898 0.     ]\n",
      " [1.13902 1.13901 1.13902 0.     ]\n",
      " [1.13916 1.13916 1.13917 0.     ]\n",
      " [1.13917 1.13917 1.13917 0.     ]\n",
      " [1.13916 1.13916 1.13917 0.     ]\n",
      " [1.13922 1.13922 1.13922 0.     ]\n",
      " [1.13922 1.13922 1.13922 0.     ]\n",
      " [1.13932 1.13932 1.13932 0.     ]\n",
      " [1.13936 1.13936 1.13936 0.     ]\n",
      " [1.13937 1.13937 1.13937 0.     ]\n",
      " [1.13931 1.13931 1.13931 0.     ]\n",
      " [1.13932 1.13932 1.13932 0.     ]\n",
      " [1.13932 1.13932 1.13933 0.     ]\n",
      " [1.13936 1.13936 1.13936 0.     ]\n",
      " [1.13937 1.13937 1.13937 0.     ]\n",
      " [1.13938 1.13938 1.13938 0.     ]\n",
      " [1.13938 1.13938 1.13938 0.     ]\n",
      " [1.13938 1.13938 1.13938 0.     ]\n",
      " [1.13937 1.13937 1.13937 0.     ]\n",
      " [1.13932 1.13932 1.13932 0.     ]\n",
      " [1.13927 1.13927 1.13927 0.     ]\n",
      " [1.13935 1.13934 1.13935 0.     ]\n",
      " [1.13937 1.13937 1.13937 0.     ]\n",
      " [1.13937 1.13937 1.13937 0.     ]\n",
      " [1.13937 1.13937 1.13937 0.     ]\n",
      " [1.13942 1.13942 1.13942 0.     ]\n",
      " [1.13948 1.13948 1.13948 0.     ]\n",
      " [1.13938 1.13938 1.13938 0.     ]\n",
      " [1.13937 1.13937 1.13937 0.     ]\n",
      " [1.13938 1.13937 1.13938 0.     ]\n",
      " [1.13947 1.13947 1.13947 0.     ]\n",
      " [1.13948 1.13947 1.13948 0.     ]\n",
      " [1.13957 1.13957 1.13957 0.     ]\n",
      " [1.13952 1.13952 1.13953 0.     ]\n",
      " [1.13947 1.13947 1.13947 0.     ]\n",
      " [1.13943 1.13942 1.13943 0.     ]\n",
      " [1.13946 1.13946 1.13946 0.     ]\n",
      " [1.13941 1.13941 1.13941 0.     ]\n",
      " [1.13946 1.13946 1.13946 0.     ]\n",
      " [1.13947 1.13947 1.13947 0.     ]\n",
      " [1.13946 1.13946 1.13947 0.     ]\n",
      " [1.13949 1.13949 1.13949 0.     ]\n",
      " [1.13953 1.13953 1.13953 0.     ]\n",
      " [1.13963 1.13963 1.13963 0.     ]\n",
      " [1.13967 1.13967 1.13967 0.     ]\n",
      " [1.13967 1.13967 1.13967 0.     ]\n",
      " [1.13963 1.13963 1.13963 0.     ]\n",
      " [1.13963 1.13963 1.13963 0.     ]\n",
      " [1.13956 1.13955 1.13956 0.     ]\n",
      " [1.13938 1.13937 1.13938 0.     ]\n",
      " [1.13942 1.13941 1.13942 0.     ]\n",
      " [1.13944 1.13944 1.13944 0.     ]\n",
      " [1.13943 1.13942 1.13943 0.     ]\n",
      " [1.13937 1.13937 1.13937 0.     ]\n",
      " [1.13942 1.13941 1.13942 0.     ]\n",
      " [1.13943 1.13942 1.13943 0.     ]\n",
      " [1.13938 1.13938 1.13939 0.     ]\n",
      " [1.13938 1.13938 1.13938 0.     ]\n",
      " [1.13942 1.13942 1.13942 0.     ]\n",
      " [1.13942 1.13942 1.13942 0.     ]\n",
      " [1.13939 1.13939 1.1394  0.     ]\n",
      " [1.13941 1.1394  1.13941 0.     ]\n",
      " [1.13943 1.13942 1.13943 0.     ]\n",
      " [1.13947 1.13947 1.13947 0.     ]\n",
      " [1.13946 1.13946 1.13947 0.     ]\n",
      " [1.13943 1.13943 1.13943 0.     ]\n",
      " [1.13946 1.13946 1.13947 0.     ]\n",
      " [1.13948 1.13948 1.13948 0.     ]\n",
      " [1.13948 1.13948 1.13948 0.     ]\n",
      " [1.13946 1.13946 1.13947 0.     ]\n",
      " [1.13947 1.13947 1.13947 0.     ]\n",
      " [1.13948 1.13948 1.13948 0.     ]\n",
      " [1.13948 1.13948 1.13948 0.     ]\n",
      " [1.13941 1.13941 1.13941 0.     ]\n",
      " [1.13936 1.13935 1.13936 0.     ]\n",
      " [1.13929 1.13929 1.13929 0.     ]]\n"
     ]
    }
   ],
   "source": [
    "print(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure all data is float\n",
    "values = values.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize features\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "scaled = scaler.fit_transform(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frame as supervised learning\n",
    "reframed = series_to_supervised(scaled, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new = reframed.tail(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   var1(t-1)  var2(t-1)  var3(t-1)  var4(t-1)   var1(t)   var2(t)   var3(t)  \\\n",
      "1   0.179810   0.179810   0.179810        0.0  0.057556  0.057556  0.057556   \n",
      "2   0.057556   0.057556   0.057556        0.0  0.000000  0.000000  0.000000   \n",
      "3   0.000000   0.000000   0.000000        0.0  0.093445  0.093445  0.100647   \n",
      "4   0.093445   0.093445   0.100647        0.0  0.719360  0.719360  0.719360   \n",
      "5   0.719360   0.719360   0.719360        0.0  0.633118  0.633118  0.640320   \n",
      "\n",
      "   var4(t)  \n",
      "1      0.0  \n",
      "2      0.0  \n",
      "3      0.0  \n",
      "4      0.0  \n",
      "5      0.0  \n"
     ]
    }
   ],
   "source": [
    "print(reframed.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns we don't want to predict\n",
    "reframed.drop(reframed.columns[[5,6,7]], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   var1(t-1)  var2(t-1)  var3(t-1)  var4(t-1)   var1(t)\n",
      "1   0.179810   0.179810   0.179810        0.0  0.057556\n",
      "2   0.057556   0.057556   0.057556        0.0  0.000000\n",
      "3   0.000000   0.000000   0.000000        0.0  0.093445\n",
      "4   0.093445   0.093445   0.100647        0.0  0.719360\n",
      "5   0.719360   0.719360   0.719360        0.0  0.633118\n"
     ]
    }
   ],
   "source": [
    "# print 1st 5 rows of transformed dataset, can observe 2 input variables (input series) and the 1 output variable (NZX50 index for next month).\n",
    "print(reframed.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test sets\n",
    "values = reframed.values\n",
    "n_train_hours = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data rows up to 660\n",
    "train = values[:n_train_hours, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data rows after 660\n",
    "test = values[n_train_hours:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data\n",
      "[[0.17980957 0.17980957 0.17980957 0.        ]\n",
      " [0.05755615 0.05755615 0.05755615 0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.09344482 0.09344482 0.10064697 0.        ]\n",
      " [0.71936035 0.71936035 0.71936035 0.        ]\n",
      " [0.6331177  0.6331177  0.6403198  0.        ]\n",
      " [0.5970459  0.5970459  0.5970459  0.        ]\n",
      " [0.7697754  0.7697754  0.7697754  0.        ]\n",
      " [0.7553711  0.7553711  0.7553711  0.        ]\n",
      " [0.60424805 0.60424805 0.60424805 0.        ]\n",
      " [0.52508545 0.52508545 0.52508545 0.        ]\n",
      " [0.45324707 0.45324707 0.45324707 0.        ]\n",
      " [0.5108032  0.5108032  0.5108032  0.        ]\n",
      " [0.35968018 0.35968018 0.35968018 0.        ]\n",
      " [0.45324707 0.45324707 0.45324707 0.        ]\n",
      " [0.41723633 0.41723633 0.41723633 0.        ]\n",
      " [0.28051758 0.28051758 0.28051758 0.        ]\n",
      " [0.32373047 0.32373047 0.32373047 0.        ]\n",
      " [0.38128662 0.37408447 0.38128662 0.        ]\n",
      " [0.41003418 0.41003418 0.41003418 0.        ]\n",
      " [0.36688232 0.36688232 0.36688232 0.        ]\n",
      " [0.43164062 0.43164062 0.43164062 0.        ]\n",
      " [0.45324707 0.44604492 0.45324707 0.        ]\n",
      " [0.45324707 0.45324707 0.45324707 0.        ]\n",
      " [0.5036011  0.5036011  0.5036011  0.        ]\n",
      " [0.53234863 0.52508545 0.53234863 0.        ]\n",
      " [0.6331177  0.6331177  0.6403198  0.        ]\n",
      " [0.6403198  0.6403198  0.6403198  0.        ]\n",
      " [0.6331177  0.6331177  0.6403198  0.        ]\n",
      " [0.6762085  0.6762085  0.6762085  0.        ]\n",
      " [0.6762085  0.6762085  0.6762085  0.        ]\n",
      " [0.74816895 0.74816895 0.74816895 0.        ]\n",
      " [0.7769165  0.7769165  0.7769165  0.        ]\n",
      " [0.78411865 0.78411865 0.78411865 0.        ]\n",
      " [0.7409668  0.7409668  0.7409668  0.        ]\n",
      " [0.74816895 0.74816895 0.74816895 0.        ]\n",
      " [0.74816895 0.74816895 0.7553711  0.        ]\n",
      " [0.7769165  0.7769165  0.7769165  0.        ]\n",
      " [0.78411865 0.78411865 0.78411865 0.        ]\n",
      " [0.7913208  0.7913208  0.7913208  0.        ]\n",
      " [0.7913208  0.7913208  0.7913208  0.        ]\n",
      " [0.7913208  0.7913208  0.7913208  0.        ]\n",
      " [0.78411865 0.78411865 0.78411865 0.        ]\n",
      " [0.74816895 0.74816895 0.74816895 0.        ]\n",
      " [0.7121582  0.7121582  0.7121582  0.        ]\n",
      " [0.7697754  0.76257324 0.7697754  0.        ]\n",
      " [0.78411865 0.78411865 0.78411865 0.        ]\n",
      " [0.78411865 0.78411865 0.78411865 0.        ]\n",
      " [0.78411865 0.78411865 0.78411865 0.        ]\n",
      " [0.8201294  0.8201294  0.8201294  0.        ]\n",
      " [0.86328125 0.86328125 0.86328125 0.        ]\n",
      " [0.7913208  0.7913208  0.7913208  0.        ]\n",
      " [0.78411865 0.78411865 0.78411865 0.        ]\n",
      " [0.7913208  0.78411865 0.7913208  0.        ]\n",
      " [0.8560791  0.8560791  0.8560791  0.        ]\n",
      " [0.86328125 0.8560791  0.86328125 0.        ]\n",
      " [0.92803955 0.92803955 0.92803955 0.        ]\n",
      " [0.89208984 0.89208984 0.89923096 0.        ]\n",
      " [0.8560791  0.8560791  0.8560791  0.        ]\n",
      " [0.82733154 0.8201294  0.82733154 0.        ]\n",
      " [0.84887695 0.84887695 0.84887695 0.        ]\n",
      " [0.81292725 0.81292725 0.81292725 0.        ]\n",
      " [0.84887695 0.84887695 0.84887695 0.        ]\n",
      " [0.8560791  0.8560791  0.8560791  0.        ]\n",
      " [0.84887695 0.84887695 0.8560791  0.        ]\n",
      " [0.8704834  0.8704834  0.8704834  0.        ]\n",
      " [0.89923096 0.89923096 0.89923096 0.        ]\n",
      " [0.9711914  0.9711914  0.9711914  0.        ]\n",
      " [1.         1.         1.         0.        ]\n",
      " [1.         1.         1.         0.        ]\n",
      " [0.9711914  0.9711914  0.9711914  0.        ]\n",
      " [0.9711914  0.9711914  0.9711914  0.        ]\n",
      " [0.9208374  0.91363525 0.9208374  0.        ]\n",
      " [0.7913208  0.78411865 0.7913208  0.        ]\n",
      " [0.8201294  0.81292725 0.8201294  0.        ]\n",
      " [0.83447266 0.83447266 0.83447266 0.        ]\n",
      " [0.82733154 0.8201294  0.82733154 0.        ]\n",
      " [0.78411865 0.78411865 0.78411865 0.        ]\n",
      " [0.8201294  0.81292725 0.8201294  0.        ]\n",
      " [0.82733154 0.8201294  0.82733154 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# split into input and outputs\n",
    "train_X, train_y = train[:, :-1], train[:, -1]\n",
    "test_X, test_y = test[:, :-1], test[:, -1]\n",
    "print('Train data')\n",
    "print(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape\n",
      "(80, 1, 4)\n"
     ]
    }
   ],
   "source": [
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
    "print('Train shape')\n",
    "print(train_X.shape)\n",
    "test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 1, 4) (80,) (19, 1, 4) (19,)\n"
     ]
    }
   ],
   "source": [
    "# prints the shape of the train and test input and output sets with about 2000 of data for training and about 671 for testing.\n",
    "print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define LSTM design network with 50 neurons in the first hidden layer and 1 neuron in the output layer for predicting pollution. The input shape will be 1 time step with 3 features. \n",
    "model = Sequential()\n",
    "model.add(LSTM(50, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the Mean Absolute Error (MAE) loss function and the efficient Adam version of stochastic gradient descent\n",
    "model.compile(loss='mse', optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80 samples, validate on 19 samples\n",
      "Epoch 1/50\n",
      " - 1s - loss: 0.5723 - val_loss: 0.6502\n",
      "Epoch 2/50\n",
      " - 0s - loss: 0.4992 - val_loss: 0.5931\n",
      "Epoch 3/50\n",
      " - 0s - loss: 0.4551 - val_loss: 0.5475\n",
      "Epoch 4/50\n",
      " - 0s - loss: 0.4198 - val_loss: 0.5078\n",
      "Epoch 5/50\n",
      " - 0s - loss: 0.3891 - val_loss: 0.4720\n",
      "Epoch 6/50\n",
      " - 0s - loss: 0.3614 - val_loss: 0.4390\n",
      "Epoch 7/50\n",
      " - 0s - loss: 0.3358 - val_loss: 0.4079\n",
      "Epoch 8/50\n",
      " - 0s - loss: 0.3118 - val_loss: 0.3786\n",
      "Epoch 9/50\n",
      " - 0s - loss: 0.2891 - val_loss: 0.3508\n",
      "Epoch 10/50\n",
      " - 0s - loss: 0.2677 - val_loss: 0.3242\n",
      "Epoch 11/50\n",
      " - 0s - loss: 0.2472 - val_loss: 0.2988\n",
      "Epoch 12/50\n",
      " - 0s - loss: 0.2277 - val_loss: 0.2745\n",
      "Epoch 13/50\n",
      " - 0s - loss: 0.2090 - val_loss: 0.2513\n",
      "Epoch 14/50\n",
      " - 0s - loss: 0.1913 - val_loss: 0.2290\n",
      "Epoch 15/50\n",
      " - 0s - loss: 0.1743 - val_loss: 0.2078\n",
      "Epoch 16/50\n",
      " - 0s - loss: 0.1582 - val_loss: 0.1876\n",
      "Epoch 17/50\n",
      " - 0s - loss: 0.1429 - val_loss: 0.1685\n",
      "Epoch 18/50\n",
      " - 0s - loss: 0.1284 - val_loss: 0.1504\n",
      "Epoch 19/50\n",
      " - 0s - loss: 0.1148 - val_loss: 0.1334\n",
      "Epoch 20/50\n",
      " - 0s - loss: 0.1020 - val_loss: 0.1174\n",
      "Epoch 21/50\n",
      " - 0s - loss: 0.0901 - val_loss: 0.1025\n",
      "Epoch 22/50\n",
      " - 0s - loss: 0.0791 - val_loss: 0.0888\n",
      "Epoch 23/50\n",
      " - 0s - loss: 0.0689 - val_loss: 0.0761\n",
      "Epoch 24/50\n",
      " - 0s - loss: 0.0596 - val_loss: 0.0645\n",
      "Epoch 25/50\n",
      " - 0s - loss: 0.0512 - val_loss: 0.0541\n",
      "Epoch 26/50\n",
      " - 0s - loss: 0.0437 - val_loss: 0.0448\n",
      "Epoch 27/50\n",
      " - 0s - loss: 0.0371 - val_loss: 0.0366\n",
      "Epoch 28/50\n",
      " - 0s - loss: 0.0313 - val_loss: 0.0295\n",
      "Epoch 29/50\n",
      " - 0s - loss: 0.0264 - val_loss: 0.0233\n",
      "Epoch 30/50\n",
      " - 0s - loss: 0.0222 - val_loss: 0.0181\n",
      "Epoch 31/50\n",
      " - 0s - loss: 0.0188 - val_loss: 0.0139\n",
      "Epoch 32/50\n",
      " - 0s - loss: 0.0161 - val_loss: 0.0104\n",
      "Epoch 33/50\n",
      " - 0s - loss: 0.0139 - val_loss: 0.0077\n",
      "Epoch 34/50\n",
      " - 0s - loss: 0.0123 - val_loss: 0.0056\n",
      "Epoch 35/50\n",
      " - 0s - loss: 0.0112 - val_loss: 0.0041\n",
      "Epoch 36/50\n",
      " - 0s - loss: 0.0104 - val_loss: 0.0030\n",
      "Epoch 37/50\n",
      " - 0s - loss: 0.0099 - val_loss: 0.0022\n",
      "Epoch 38/50\n",
      " - 0s - loss: 0.0096 - val_loss: 0.0017\n",
      "Epoch 39/50\n",
      " - 0s - loss: 0.0094 - val_loss: 0.0014\n",
      "Epoch 40/50\n",
      " - 0s - loss: 0.0093 - val_loss: 0.0012\n",
      "Epoch 41/50\n",
      " - 0s - loss: 0.0093 - val_loss: 0.0010\n",
      "Epoch 42/50\n",
      " - 0s - loss: 0.0093 - val_loss: 9.6304e-04\n",
      "Epoch 43/50\n",
      " - 0s - loss: 0.0093 - val_loss: 9.1464e-04\n",
      "Epoch 44/50\n",
      " - 0s - loss: 0.0092 - val_loss: 8.8545e-04\n",
      "Epoch 45/50\n",
      " - 0s - loss: 0.0092 - val_loss: 8.6723e-04\n",
      "Epoch 46/50\n",
      " - 0s - loss: 0.0092 - val_loss: 8.5462e-04\n",
      "Epoch 47/50\n",
      " - 0s - loss: 0.0092 - val_loss: 8.4447e-04\n",
      "Epoch 48/50\n",
      " - 0s - loss: 0.0092 - val_loss: 8.3501e-04\n",
      "Epoch 49/50\n",
      " - 0s - loss: 0.0091 - val_loss: 8.2523e-04\n",
      "Epoch 50/50\n",
      " - 0s - loss: 0.0091 - val_loss: 8.1486e-04\n"
     ]
    }
   ],
   "source": [
    "# fit network. The model will be fit for 262 training epochs with a batch size of 300 and  keep track of both the training and test loss during training by setting the validation_data argument in the fit() function.\n",
    "\n",
    "history = model.fit(train_X, train_y, epochs=50, batch_size=72, validation_data=(test_X, test_y), verbose=2, shuffle= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# At the end of the run both the training and test loss are plotted.\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After the model is fit, model can be used to forecast for the entire test dataset.\n",
    " \n",
    "yhat = model.predict(test_X)\n",
    "test_X = test_X.reshape((test_X.shape[0], test_X.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invert scaling for forecast \n",
    "#Combine the forecast with the test dataset and invert the scaling. Then invert scaling on the test dataset with the expected NZX50 numbers.\n",
    "\n",
    "inv_yhat = concatenate((test_X[:, 1:],yhat), axis=1)\n",
    "inv_yhat = scaler.inverse_transform(inv_yhat)\n",
    "inv_yhat = inv_yhat[:, 0]\n",
    "\n",
    "# inv_yhat = concatenate((yhat, test_X[:, 1:]), axis=1)\n",
    "# inv_yhat = scaler.inverse_transform(inv_yhat)\n",
    "# inv_yhat = inv_yhat[:,0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yhat x\n",
      "[1.13938 1.13938 1.13942 1.13942 1.13939 1.1394  1.13942 1.13947 1.13946\n",
      " 1.13943 1.13946 1.13948 1.13948 1.13946 1.13947 1.13948 1.13948 1.13941\n",
      " 1.13935]\n"
     ]
    }
   ],
   "source": [
    "#Print predicted values for test data\n",
    "print('yhat x')\n",
    "print(inv_yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invert scaling for actual\n",
    "test_y = test_y.reshape((len(test_y), 1))\n",
    "inv_y = concatenate((test_X[:, 1:],test_y), axis=1)\n",
    "inv_y = scaler.inverse_transform(inv_y)\n",
    "inv_y = inv_y[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 0.000\n"
     ]
    }
   ],
   "source": [
    "# calculate RMSE\n",
    "#With forecasts and actual values in their original scale, it is possible to calculate an error score for the model. In this case, calculate the Root Mean Squared Error (RMSE) that gives error in the same units as the variable itself\n",
    "\n",
    "rmse = sqrt(mean_squared_error(inv_y, inv_yhat))\n",
    "print('Test RMSE: %.3f' %rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
